{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 22:51:25.564422: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-01 22:51:26.142661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/yisan/anaconda3/envs/gpu_env/lib/\n",
      "2023-02-01 22:51:26.142709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/yisan/anaconda3/envs/gpu_env/lib/\n",
      "2023-02-01 22:51:26.142713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 9292\n",
    "torch.manual_seed(random_seed)\n",
    "LATENT_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "lr = 5e-5\n",
    "latent_dims = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training = CIFAR10(data_dir, train=True, transform=transform, download=True)\n",
    "testing = CIFAR10(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(testing, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FrechetInceptionDistance(features=2048, normalize=True, reset_real_features=False).to(device)\n",
    "\n",
    "for i, (real_imgs, _) in enumerate(test_dataloader):\n",
    "    real_imgs = real_imgs.float().cuda()\n",
    "    fid.update(real_imgs, real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models.wgan import WGANDiscriminator, WGANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(disc, gen, disc_optim, gen_optim, basedir, train_dataloader, test_dataloader, fid):\n",
    "    disc.train()\n",
    "    gen.train()\n",
    "    train_num_batches = len(train_dataloader)\n",
    "    dict_result = {}\n",
    "    dict_result['avg_wasserstein_d'], dict_result['avg_disc_loss'], dict_result['avg_gen_loss'], dict_result['avg_real_acc'], dict_result['avg_fake_acc'] = [], [], [], [], []\n",
    "    dict_result['test_wasserstein_d'], dict_result['test_disc_loss'], dict_result['test_gen_loss'], dict_result['test_real_acc'], dict_result['test_fake_acc'], dict_result['fid_score'] = [], [], [], [], [], []\n",
    "\n",
    "    one = torch.FloatTensor([1]).to(device)\n",
    "    minus_one = one * -1\n",
    "\n",
    "    test_gan(gen, disc, dict_result, 0, basedir, test_dataloader, fid)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_wasserstein_d, epoch_disc_loss, epoch_gen_loss, epoch_real_acc, epoch_fake_acc = [], [], [], [], []\n",
    "        for i, (real_imgs, _) in enumerate(train_dataloader):\n",
    "            real_imgs = real_imgs.float().cuda()\n",
    "            noise = torch.randn(LATENT_DIM*real_imgs.shape[0]).to(device)\n",
    "            fake_imgs = gen(noise.reshape(real_imgs.shape[0], LATENT_DIM, 1, 1))\n",
    "\n",
    "            disc.zero_grad()\n",
    "\n",
    "            disc_real = disc(real_imgs)\n",
    "            disc_fake = disc(fake_imgs)\n",
    "\n",
    "            real_acc = torch.sum(torch.round(disc_real) == torch.ones_like(disc_real)) / len(disc_real)\n",
    "            fake_acc = torch.sum(torch.round(disc_fake) == torch.zeros_like(disc_fake)) / len(disc_fake)\n",
    "        \n",
    "            real_loss = disc_real.mean(0).view(1)\n",
    "            real_loss.backward(one)\n",
    "\n",
    "            fake_loss = disc_fake.mean(0).view(1)\n",
    "            fake_loss.backward(minus_one)\n",
    "\n",
    "            disc_optim.step()\n",
    "\n",
    "            disc_loss = fake_loss - real_loss\n",
    "            wasserstein_d = real_loss - fake_loss\n",
    "\n",
    "            epoch_wasserstein_d.append(wasserstein_d)\n",
    "            epoch_disc_loss.append(disc_loss.item()) \n",
    "            epoch_real_acc.append(real_acc.item()) \n",
    "            epoch_fake_acc.append(fake_acc.item()) \n",
    "\n",
    "            gen.zero_grad()\n",
    "\n",
    "            noise = torch.randn(LATENT_DIM*real_imgs.shape[0]).to(device)\n",
    "            fake_imgs = gen(noise.reshape(real_imgs.shape[0], LATENT_DIM, 1, 1))\n",
    "\n",
    "            disc_fake = disc(fake_imgs)\n",
    "            gen_loss = disc_fake.mean().mean(0).view(1)\n",
    "            # gen_loss = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "            gen_loss.backward(one)\n",
    "            gen_optim.step()\n",
    "\n",
    "            epoch_gen_loss.append(gen_loss.item())\n",
    "\n",
    "            progress = int(((i+1)/train_num_batches)*20)\n",
    "            print(f\"{epoch+1}/{NUM_EPOCHS} [{'='*progress}{' '*(20-progress)}] {i+1}/{train_num_batches}\", end='\\r', flush=True)\n",
    "\n",
    "            del disc_loss, gen_loss, real_acc, fake_acc, noise, fake_imgs, disc_real, disc_fake, real_loss, fake_loss, wasserstein_d\n",
    "\n",
    "        # dcgan_agent.output_train_graphs('.')\n",
    "        \n",
    "        avg_wasserstein_d = sum(epoch_wasserstein_d) / len(epoch_wasserstein_d)\n",
    "        avg_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)\n",
    "        avg_gen_loss = sum(epoch_gen_loss) / len(epoch_gen_loss)\n",
    "        avg_real_acc = sum(epoch_real_acc) / len(epoch_real_acc)\n",
    "        avg_fake_acc = sum(epoch_fake_acc) / len(epoch_fake_acc)\n",
    "\n",
    "        dict_result['avg_wasserstein_d'].append(avg_wasserstein_d)\n",
    "        dict_result['avg_disc_loss'].append(avg_disc_loss)\n",
    "        dict_result['avg_gen_loss'].append(avg_gen_loss)\n",
    "        dict_result['avg_real_acc'].append(avg_real_acc)\n",
    "        dict_result['avg_fake_acc'].append(avg_fake_acc)\n",
    "\n",
    "        del epoch_disc_loss, epoch_gen_loss, epoch_real_acc, epoch_fake_acc, avg_wasserstein_d\n",
    "\n",
    "        plot_training_graphs(basedir, dict_result['avg_disc_loss'], dict_result['avg_gen_loss'], dict_result['avg_real_acc'], dict_result['avg_fake_acc'])\n",
    "\n",
    "        print(f'\\ndisc loss: {avg_disc_loss:5.2f}  gen loss: {avg_gen_loss:5.2f}  real acc: {avg_real_acc:5.2f}  fake_acc: {avg_fake_acc:5.2f}\\n')\n",
    "        del avg_disc_loss, avg_gen_loss, avg_real_acc, avg_fake_acc\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            test_gan(gen, disc, dict_result, epoch, basedir, test_dataloader, fid)\n",
    "            torch.save(gen.state_dict(), basedir+f'/saves/model{str(epoch+1)}.pt')\n",
    "\n",
    "    return dict_result\n",
    "\n",
    "def test_gan(gen, disc, dict_result, epoch, basedir, test_dataloader, fid):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_wasserstein_d, epoch_disc_loss, epoch_gen_loss, epoch_real_acc, epoch_fake_acc = [], [], [], [], []\n",
    "\n",
    "        for i, (real_imgs, _) in enumerate(test_dataloader):\n",
    "            real_imgs = real_imgs.float().cuda()\n",
    "            noise = torch.randn(LATENT_DIM*real_imgs.shape[0]).to(device)\n",
    "            fake_imgs = gen(noise.reshape(real_imgs.shape[0], LATENT_DIM, 1, 1))\n",
    "\n",
    "            disc.zero_grad()\n",
    "\n",
    "            disc_real = disc(real_imgs)\n",
    "            disc_fake = disc(fake_imgs)\n",
    "\n",
    "            real_acc = torch.sum(torch.round(disc_real) == torch.ones_like(disc_real)) / len(disc_real)\n",
    "            fake_acc = torch.sum(torch.round(disc_fake) == torch.zeros_like(disc_fake)) / len(disc_fake)\n",
    "        \n",
    "            real_loss = disc_real.mean(0).view(1)\n",
    "\n",
    "            fake_loss = disc_fake.mean(0).view(1)\n",
    "\n",
    "            disc_loss = fake_loss - real_loss\n",
    "            wasserstein_d = real_loss - fake_loss\n",
    "\n",
    "            disc_fake = disc(fake_imgs)\n",
    "            gen_loss = disc_fake.mean().mean(0).view(1)\n",
    "\n",
    "            fid.update(fake_imgs, real=False)\n",
    "\n",
    "            epoch_wasserstein_d.append(wasserstein_d)\n",
    "            epoch_disc_loss.append(disc_loss.item())\n",
    "            epoch_gen_loss.append(gen_loss.item())\n",
    "            epoch_real_acc.append(real_acc.item())\n",
    "            epoch_fake_acc.append(fake_acc.item())\n",
    "\n",
    "            progress = int(((i+1)/len(test_dataloader))*20)\n",
    "            print(f\"test [{'='*progress}{' '*(20-progress)}] {i+1}/{len(test_dataloader)}\", end='\\r', flush=True)\n",
    "\n",
    "            del disc_loss, gen_loss, real_acc, fake_acc, noise, fake_imgs, disc_real, disc_fake, wasserstein_d\n",
    "\n",
    "        test_wasserstein_d = sum(epoch_wasserstein_d) / len(epoch_wasserstein_d)\n",
    "        test_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)\n",
    "        test_gen_loss = sum(epoch_gen_loss) / len(epoch_gen_loss)\n",
    "        test_real_acc = sum(epoch_real_acc) / len(epoch_real_acc)\n",
    "        test_fake_acc = sum(epoch_fake_acc) / len(epoch_fake_acc)\n",
    "        dict_result['test_wasserstein_d'].append(test_wasserstein_d)\n",
    "        dict_result['test_disc_loss'].append(test_disc_loss)\n",
    "        dict_result['test_gen_loss'].append(test_gen_loss)\n",
    "        dict_result['test_real_acc'].append(test_real_acc)\n",
    "        dict_result['test_fake_acc'].append(test_fake_acc)\n",
    "        del epoch_gen_loss, epoch_disc_loss, epoch_real_acc, epoch_fake_acc, test_wasserstein_d\n",
    "\n",
    "        test_graphs(basedir, dict_result)\n",
    "\n",
    "        fid_score = fid.compute().item()\n",
    "        fid.reset()\n",
    "        dict_result['fid_score'].append(fid_score)\n",
    "\n",
    "        output_imgs(basedir, gen, epoch, test_disc_loss, test_gen_loss, fid_score)\n",
    "        \n",
    "        gen.train()\n",
    "        disc.train()\n",
    "        del fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 [====================] 782/782\n",
      "disc loss: 2903.33  gen loss: 1512.60  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "2/100 [====================] 782/782\n",
      "disc loss: 11285.43  gen loss: 5867.70  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "3/100 [====================] 782/782\n",
      "disc loss: 21235.15  gen loss: 11444.08  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "4/100 [====================] 782/782\n",
      "disc loss: 35339.10  gen loss: 18881.04  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "5/100 [====================] 782/782\n",
      "disc loss: 53371.90  gen loss: 27988.57  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "6/100 [====================] 782/782\n",
      "disc loss: 63399.83  gen loss: 35844.41  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "7/100 [====================] 782/782\n",
      "disc loss: 73323.41  gen loss: 42391.12  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "8/100 [====================] 782/782\n",
      "disc loss: 82263.77  gen loss: 46809.58  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "9/100 [====================] 782/782\n",
      "disc loss: 90613.78  gen loss: 52858.47  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "10/100 [====================] 782/782\n",
      "disc loss: 98776.54  gen loss: 60753.81  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "11/100 [====================] 782/782\n",
      "disc loss: 100209.99  gen loss: 65422.39  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "12/100 [====================] 782/782\n",
      "disc loss: 102437.98  gen loss: 69493.32  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "13/100 [====================] 782/782\n",
      "disc loss: 103425.55  gen loss: 69896.92  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "14/100 [====================] 782/782\n",
      "disc loss: 106514.74  gen loss: 76618.63  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "15/100 [====================] 782/782\n",
      "disc loss: 109206.80  gen loss: 81879.06  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "16/100 [====================] 782/782\n",
      "disc loss: 114188.10  gen loss: 85422.18  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "17/100 [====================] 782/782\n",
      "disc loss: 118397.90  gen loss: 91342.57  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "18/100 [====================] 782/782\n",
      "disc loss: 123632.63  gen loss: 97150.04  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "19/100 [====================] 782/782\n",
      "disc loss: 124176.66  gen loss: 98802.69  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "20/100 [====================] 782/782\n",
      "disc loss: 126145.54  gen loss: 107542.99  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "21/100 [====================] 782/782\n",
      "disc loss: 123069.65  gen loss: 107822.51  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "22/100 [====================] 782/782\n",
      "disc loss: 122902.74  gen loss: 108788.12  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "23/100 [====================] 782/782\n",
      "disc loss: 124031.06  gen loss: 115555.33  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "24/100 [====================] 782/782\n",
      "disc loss: 130315.64  gen loss: 118610.74  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "25/100 [====================] 782/782\n",
      "disc loss: 133000.37  gen loss: 126579.03  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "26/100 [====================] 782/782\n",
      "disc loss: 146404.75  gen loss: 133307.01  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "27/100 [====================] 782/782\n",
      "disc loss: 155452.26  gen loss: 143508.16  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "28/100 [====================] 782/782\n",
      "disc loss: 176208.92  gen loss: 151913.86  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "29/100 [====================] 782/782\n",
      "disc loss: 196825.70  gen loss: 161021.36  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "30/100 [====================] 782/782\n",
      "disc loss: 223632.48  gen loss: 165085.40  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "31/100 [====================] 782/782\n",
      "disc loss: 236750.61  gen loss: 150604.31  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "32/100 [====================] 782/782\n",
      "disc loss: 255595.68  gen loss: 153277.39  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "33/100 [====================] 782/782\n",
      "disc loss: 293540.91  gen loss: 183977.96  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "34/100 [====================] 782/782\n",
      "disc loss: 310737.07  gen loss: 184088.64  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "35/100 [====================] 782/782\n",
      "disc loss: 338250.24  gen loss: 199245.64  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "36/100 [====================] 782/782\n",
      "disc loss: 359656.30  gen loss: 199334.12  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "37/100 [====================] 782/782\n",
      "disc loss: 399107.67  gen loss: 236469.32  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "38/100 [====================] 782/782\n",
      "disc loss: 402318.62  gen loss: 237513.63  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "39/100 [====================] 782/782\n",
      "disc loss: 428770.29  gen loss: 243991.53  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "40/100 [====================] 782/782\n",
      "disc loss: 448229.68  gen loss: 247440.76  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "41/100 [====================] 782/782\n",
      "disc loss: 479845.78  gen loss: 275198.95  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "42/100 [====================] 782/782\n",
      "disc loss: 538160.78  gen loss: 305523.23  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "43/100 [====================] 782/782\n",
      "disc loss: 569153.78  gen loss: 299022.93  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "44/100 [====================] 782/782\n",
      "disc loss: 604776.18  gen loss: 333527.25  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "45/100 [====================] 782/782\n",
      "disc loss: 619113.12  gen loss: 336035.45  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "46/100 [====================] 782/782\n",
      "disc loss: 670650.38  gen loss: 378075.47  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "47/100 [====================] 782/782\n",
      "disc loss: 664008.49  gen loss: 377021.25  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "48/100 [====================] 782/782\n",
      "disc loss: 720806.18  gen loss: 409607.60  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "49/100 [====================] 782/782\n",
      "disc loss: 766949.20  gen loss: 440661.43  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "50/100 [====================] 782/782\n",
      "disc loss: 827846.55  gen loss: 456342.94  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "51/100 [====================] 782/782\n",
      "disc loss: 826998.87  gen loss: 448079.05  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "52/100 [====================] 782/782\n",
      "disc loss: 892644.39  gen loss: 504398.60  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "53/100 [====================] 782/782\n",
      "disc loss: 990873.23  gen loss: 538302.67  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "54/100 [====================] 782/782\n",
      "disc loss: 980255.54  gen loss: 526994.98  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "55/100 [====================] 782/782\n",
      "disc loss: 1062206.87  gen loss: 594645.56  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "56/100 [====================] 782/782\n",
      "disc loss: 1063618.42  gen loss: 617483.75  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "57/100 [====================] 782/782\n",
      "disc loss: 1144520.63  gen loss: 635685.06  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "58/100 [====================] 782/782\n",
      "disc loss: 1229001.01  gen loss: 686253.79  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "59/100 [====================] 782/782\n",
      "disc loss: 1242210.77  gen loss: 692931.43  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "60/100 [====================] 782/782\n",
      "disc loss: 1295283.18  gen loss: 726691.57  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "61/100 [====================] 782/782\n",
      "disc loss: 1311181.71  gen loss: 751473.61  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "62/100 [====================] 782/782\n",
      "disc loss: 1417311.64  gen loss: 805900.32  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "63/100 [====================] 782/782\n",
      "disc loss: 1355511.08  gen loss: 749137.70  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "64/100 [====================] 782/782\n",
      "disc loss: 1576769.06  gen loss: 903876.73  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "65/100 [====================] 782/782\n",
      "disc loss: 1580075.96  gen loss: 876802.16  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "66/100 [====================] 782/782\n",
      "disc loss: 1613704.66  gen loss: 945792.08  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "67/100 [====================] 782/782\n",
      "disc loss: 1742612.18  gen loss: 1010348.68  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "68/100 [====================] 782/782\n",
      "disc loss: 1794388.64  gen loss: 1005845.04  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "69/100 [====================] 782/782\n",
      "disc loss: 1801636.18  gen loss: 1019203.16  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "70/100 [====================] 782/782\n",
      "disc loss: 1800321.01  gen loss: 1063255.92  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "71/100 [====================] 782/782\n",
      "disc loss: 1995759.82  gen loss: 1096960.20  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "72/100 [====================] 782/782\n",
      "disc loss: 2066422.71  gen loss: 1130992.33  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "73/100 [====================] 782/782\n",
      "disc loss: 2036883.18  gen loss: 1108686.85  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "74/100 [====================] 782/782\n",
      "disc loss: 2148161.76  gen loss: 1253480.51  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "75/100 [====================] 782/782\n",
      "disc loss: 2248934.51  gen loss: 1218160.95  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "76/100 [====================] 782/782\n",
      "disc loss: 2213252.99  gen loss: 1311785.24  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "77/100 [====================] 782/782\n",
      "disc loss: 2074178.67  gen loss: 1225236.64  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "78/100 [====================] 782/782\n",
      "disc loss: 2352199.93  gen loss: 1336163.70  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "79/100 [====================] 782/782\n",
      "disc loss: 2794434.41  gen loss: 1603953.21  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "80/100 [====================] 782/782\n",
      "disc loss: 2487466.75  gen loss: 1350278.54  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "81/100 [====================] 782/782\n",
      "disc loss: 2685178.09  gen loss: 1583066.46  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "82/100 [====================] 782/782\n",
      "disc loss: 2886940.43  gen loss: 1610974.65  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "83/100 [====================] 782/782\n",
      "disc loss: 2754282.96  gen loss: 1639729.46  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "84/100 [====================] 782/782\n",
      "disc loss: 2770937.87  gen loss: 1606930.94  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "85/100 [====================] 782/782\n",
      "disc loss: 2979114.17  gen loss: 1718071.34  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "86/100 [====================] 782/782\n",
      "disc loss: 2678991.43  gen loss: 1664253.03  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "87/100 [====================] 782/782\n",
      "disc loss: 2853947.28  gen loss: 1515400.93  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "88/100 [====================] 782/782\n",
      "disc loss: 3015294.36  gen loss: 1836100.21  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "89/100 [====================] 782/782\n",
      "disc loss: 3075752.87  gen loss: 1695576.36  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "90/100 [====================] 782/782\n",
      "disc loss: 3400253.25  gen loss: 1864881.25  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "91/100 [====================] 782/782\n",
      "disc loss: 3191864.59  gen loss: 1809593.98  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "92/100 [====================] 782/782\n",
      "disc loss: 3201378.57  gen loss: 1835892.18  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "93/100 [====================] 782/782\n",
      "disc loss: 3360877.22  gen loss: 1875942.21  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "94/100 [====================] 782/782\n",
      "disc loss: 3645643.11  gen loss: 2104848.68  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "95/100 [====================] 782/782\n",
      "disc loss: 3086115.43  gen loss: 1580344.85  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "96/100 [====================] 782/782\n",
      "disc loss: 3841581.17  gen loss: 1875786.37  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "97/100 [====================] 782/782\n",
      "disc loss: 3532384.67  gen loss: 1745821.97  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "98/100 [====================] 782/782\n",
      "disc loss: 4166137.41  gen loss: 2301259.07  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "99/100 [====================] 782/782\n",
      "disc loss: 3741223.25  gen loss: 2071769.89  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "100/100 [====================] 782/782\n",
      "disc loss: 4234941.57  gen loss: 2465877.84  real acc:  0.00  fake_acc:  0.00\n",
      "\n",
      "test [====================] 157/157\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_wasserstein_d': [tensor([-2903.3267], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-11285.4355], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-21235.1602], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-35339.1172], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-53371.8672], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-63399.8086], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-73323.4453], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-82263.7969], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-90613.7188], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-98776.5078], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-100210.0781], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-102437.9609], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-103425.5938], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-106514.7344], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-109206.7891], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-114188.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-118397.7891], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-123632.6719], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-124176.6953], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-126145.6250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-123069.7188], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-122902.7031], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-124031.1094], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-130315.5859], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-133000.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-146404.7656], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-155452.3281], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-176208.9375], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-196825.5938], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-223632.5781], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-236750.5625], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-255595.7188], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-293540.9062], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-310737.0312], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-338250.1562], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-359656.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-399107.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-402318.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-428770.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-448229.5312], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-479845.5938], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-538160.6250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-569154.1875], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-604776.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-619113.0625], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-670650.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-664008.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-720806.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-766948.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-827846.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-826998.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-892644.3125], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-990873.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-980255.8125], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1062206.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1063618.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1144521.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1229000.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1242211.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1295283.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1311182.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1417311.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1355511.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1576768.6250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1580076.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1613705.3750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1742612.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1794388.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1801637.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1800322.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-1995759.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2066422.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2036881.1250], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2148161.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2248935.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2213251.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2074178.8750], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2352199.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2794435.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2487465.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2685177.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2886938.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2754283.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2770939.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2979115.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2678990.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-2853948.7500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3015293.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3075752.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3400255.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3191866.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3201375.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3360878.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3645644.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3086114.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3841581.2500], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3532382.5000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-4166138.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-3741224.], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([-4234942.], device='cuda:0', grad_fn=<DivBackward0>)],\n",
       " 'avg_disc_loss': [2903.3277254528402,\n",
       "  11285.428318501738,\n",
       "  21235.153611907568,\n",
       "  35339.10169737053,\n",
       "  53371.90374240729,\n",
       "  63399.82538388147,\n",
       "  73323.41499285685,\n",
       "  82263.77392515686,\n",
       "  90613.77558044277,\n",
       "  98776.5405810422,\n",
       "  100209.98933024297,\n",
       "  102437.97804607576,\n",
       "  103425.55442275415,\n",
       "  106514.7423972986,\n",
       "  109206.8039282289,\n",
       "  114188.09698189738,\n",
       "  118397.89780310901,\n",
       "  123632.63261269181,\n",
       "  124176.65906729539,\n",
       "  126145.5387328165,\n",
       "  123069.65084019341,\n",
       "  122902.74471007833,\n",
       "  124031.05517703005,\n",
       "  130315.64117946771,\n",
       "  133000.36798173754,\n",
       "  146404.75362152333,\n",
       "  155452.26235314098,\n",
       "  176208.91998181745,\n",
       "  196825.69774766226,\n",
       "  223632.48299632352,\n",
       "  236750.61323129796,\n",
       "  255595.67788922635,\n",
       "  293540.91018622124,\n",
       "  310737.0650075927,\n",
       "  338250.2400195812,\n",
       "  359656.3017203485,\n",
       "  399107.667089594,\n",
       "  402318.6152893223,\n",
       "  428770.2882832481,\n",
       "  448229.67585118284,\n",
       "  479845.7797214674,\n",
       "  538160.7780630594,\n",
       "  569153.7812300192,\n",
       "  604776.1782788523,\n",
       "  619113.1162084399,\n",
       "  670650.3822630275,\n",
       "  664008.4947850064,\n",
       "  720806.1820152653,\n",
       "  766949.2047234655,\n",
       "  827846.5505514706,\n",
       "  826998.8666080562,\n",
       "  892644.388866688,\n",
       "  990873.2280011189,\n",
       "  980255.5416100543,\n",
       "  1062206.8701846227,\n",
       "  1063618.4194373402,\n",
       "  1144520.625439578,\n",
       "  1229001.0133871483,\n",
       "  1242210.7708300033,\n",
       "  1295283.1772498402,\n",
       "  1311181.7071611253,\n",
       "  1417311.6395859974,\n",
       "  1355511.0784846547,\n",
       "  1576769.0617107577,\n",
       "  1580075.9610773658,\n",
       "  1613704.6577085997,\n",
       "  1742612.1805266943,\n",
       "  1794388.6387867648,\n",
       "  1801636.1822850064,\n",
       "  1800321.005594629,\n",
       "  1995759.824608376,\n",
       "  2066422.7089194374,\n",
       "  2036883.1803069054,\n",
       "  2148161.7647158727,\n",
       "  2248934.511189258,\n",
       "  2213252.994849944,\n",
       "  2074178.6659506874,\n",
       "  2352199.9279891304,\n",
       "  2794434.407888427,\n",
       "  2487466.754955243,\n",
       "  2685178.093110614,\n",
       "  2886940.428628517,\n",
       "  2754282.955103101,\n",
       "  2770937.8742007674,\n",
       "  2979114.1687180307,\n",
       "  2678991.4273297633,\n",
       "  2853947.283008312,\n",
       "  3015294.361932545,\n",
       "  3075752.8665680946,\n",
       "  3400253.2466032607,\n",
       "  3191864.5933503835,\n",
       "  3201378.5721707162,\n",
       "  3360877.224904092,\n",
       "  3645643.1137308185,\n",
       "  3086115.4334638747,\n",
       "  3841581.1735733696,\n",
       "  3532384.6665201406,\n",
       "  4166137.4140025578,\n",
       "  3741223.2524776217,\n",
       "  4234941.57420876],\n",
       " 'avg_gen_loss': [1512.6018313852037,\n",
       "  5867.703511055168,\n",
       "  11444.079347771452,\n",
       "  18881.0425326062,\n",
       "  27988.56843227377,\n",
       "  35844.41115224697,\n",
       "  42391.12144247772,\n",
       "  46809.5777402912,\n",
       "  52858.46979235871,\n",
       "  60753.80665506426,\n",
       "  65422.385400796484,\n",
       "  69493.31831863287,\n",
       "  69896.9226430127,\n",
       "  76618.6321088786,\n",
       "  81879.06165323476,\n",
       "  85422.18309001483,\n",
       "  91342.57157347696,\n",
       "  97150.04023364132,\n",
       "  98802.69202409437,\n",
       "  107542.99405789192,\n",
       "  107822.50540918218,\n",
       "  108788.11966699468,\n",
       "  115555.32695300013,\n",
       "  118610.74350841941,\n",
       "  126579.03153503886,\n",
       "  133307.01426958246,\n",
       "  143508.1600391749,\n",
       "  151913.8562055427,\n",
       "  161021.36420723607,\n",
       "  165085.39569419608,\n",
       "  150604.3098893812,\n",
       "  153277.3909515615,\n",
       "  183977.96279883813,\n",
       "  184088.636067188,\n",
       "  199245.63620361954,\n",
       "  199334.11542481717,\n",
       "  236469.31962128857,\n",
       "  237513.63200015484,\n",
       "  243991.53294274997,\n",
       "  247440.76200784746,\n",
       "  275198.95110194216,\n",
       "  305523.2271681686,\n",
       "  299022.93123851105,\n",
       "  333527.2511407799,\n",
       "  336035.4508665431,\n",
       "  378075.472470803,\n",
       "  377021.24571536324,\n",
       "  409607.59996028815,\n",
       "  440661.42561016424,\n",
       "  456342.93672824086,\n",
       "  448079.0489155411,\n",
       "  504398.59873396537,\n",
       "  538302.6745824009,\n",
       "  526994.9766698969,\n",
       "  594645.5618924582,\n",
       "  617483.7522640765,\n",
       "  635685.0623513927,\n",
       "  686253.7886341612,\n",
       "  692931.4305866369,\n",
       "  726691.5671730139,\n",
       "  751473.6062629875,\n",
       "  805900.3193309424,\n",
       "  749137.6963227801,\n",
       "  903876.7318249481,\n",
       "  876802.1564348226,\n",
       "  945792.07671885,\n",
       "  1010348.6825859675,\n",
       "  1005845.0358830523,\n",
       "  1019203.1609754636,\n",
       "  1063255.9195004045,\n",
       "  1096960.2028939717,\n",
       "  1130992.331881394,\n",
       "  1108686.85238546,\n",
       "  1253480.5093560182,\n",
       "  1218160.9526629436,\n",
       "  1311785.235977212,\n",
       "  1225236.6389491088,\n",
       "  1336163.7007160627,\n",
       "  1603953.2130891844,\n",
       "  1350278.5436980498,\n",
       "  1583066.4588744806,\n",
       "  1610974.651132413,\n",
       "  1639729.4638097426,\n",
       "  1606930.9422579324,\n",
       "  1718071.3430881354,\n",
       "  1664253.0262622882,\n",
       "  1515400.934203165,\n",
       "  1836100.2127832281,\n",
       "  1695576.358545796,\n",
       "  1864881.2487911605,\n",
       "  1809593.9782558742,\n",
       "  1835892.1829393783,\n",
       "  1875942.2111373083,\n",
       "  2104848.6793503235,\n",
       "  1580344.846532329,\n",
       "  1875786.3675221787,\n",
       "  1745821.9661774696,\n",
       "  2301259.0683119204,\n",
       "  2071769.8854199967,\n",
       "  2465877.8376683383],\n",
       " 'avg_real_acc': [0.00023976982097186701,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'avg_fake_acc': [0.000559462915601023,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9980818414322252e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'test_wasserstein_d': [tensor([-0.0144], device='cuda:0'),\n",
       "  tensor([-67049.5547], device='cuda:0'),\n",
       "  tensor([-84355.3516], device='cuda:0'),\n",
       "  tensor([-77029.5625], device='cuda:0'),\n",
       "  tensor([-212700.3281], device='cuda:0'),\n",
       "  tensor([-229386.2188], device='cuda:0'),\n",
       "  tensor([-542530.8125], device='cuda:0'),\n",
       "  tensor([-369869.4375], device='cuda:0'),\n",
       "  tensor([-652303.0625], device='cuda:0'),\n",
       "  tensor([-1162956.1250], device='cuda:0'),\n",
       "  tensor([-645508.8750], device='cuda:0')],\n",
       " 'test_disc_loss': [0.01441824460508907,\n",
       "  67049.55983777867,\n",
       "  84355.34472034236,\n",
       "  77029.57882165605,\n",
       "  212700.34464570065,\n",
       "  229386.16242038217,\n",
       "  542530.7669187898,\n",
       "  369869.4848726115,\n",
       "  652302.8775875797,\n",
       "  1162956.1584394905,\n",
       "  645509.0652866242],\n",
       " 'test_gen_loss': [-0.020513135951700484,\n",
       "  74769.58957006369,\n",
       "  -15890.96248787072,\n",
       "  351045.9221735669,\n",
       "  60548.59852395999,\n",
       "  939832.5473726115,\n",
       "  37264.58410877787,\n",
       "  1490434.9968152866,\n",
       "  847509.9613853503,\n",
       "  1882855.5326433121,\n",
       "  3414955.4570063693],\n",
       " 'test_real_acc': [0.0,\n",
       "  9.952229299363058e-05,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'test_fake_acc': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'fid_score': [322.9747619628906,\n",
       "  61.06822967529297,\n",
       "  23.87320899963379,\n",
       "  20.870986938476562,\n",
       "  19.6378116607666,\n",
       "  19.354106903076172,\n",
       "  18.46297836303711,\n",
       "  19.338937759399414,\n",
       "  19.187114715576172,\n",
       "  20.137365341186523,\n",
       "  21.280675888061523]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = WGANDiscriminator().to(device)\n",
    "gen = WGANGenerator(LATENT_DIM).to(device)\n",
    "\n",
    "disc.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "disc_optim = optim.RMSprop(disc.parameters(), lr=lr)\n",
    "gen_optim = optim.RMSprop(gen.parameters(), lr=lr)\n",
    "\n",
    "train_gan(disc, gen, disc_optim, gen_optim, '../model_saves/WGAN', train_dataloader, test_dataloader, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26fea177ace9e3e7cee7606ae3feec6943651aaa97caf269fed74c62b3820c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
